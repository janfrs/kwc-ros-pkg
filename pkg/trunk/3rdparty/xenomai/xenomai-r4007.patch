diff -Naur orig/include/asm-x86/atomic_32.h ./include/asm-x86/atomic_32.h
--- orig/include/asm-x86/atomic_32.h	2008-07-18 14:34:24.606068774 -0700
+++ ./include/asm-x86/atomic_32.h	2008-07-18 14:35:44.694819093 -0700
@@ -36,8 +36,8 @@
 #define xnarch_atomic_set_mask(pflags,mask)    atomic_set_mask(mask,pflags)
 #define xnarch_atomic_clear_mask(pflags,mask)  atomic_clear_mask(mask,pflags)
 #define xnarch_atomic_xchg(ptr,x)              xchg(ptr,x)
-#define xnarch_atomic_cmpxchg(pcounter,old,new) \
-	atomic_cmpxchg((pcounter),(old),(new))
+#define xnarch_atomic_cmpxchg(pcounter,old,New) \
+	atomic_cmpxchg((pcounter),(old),(New))
 
 #define xnarch_memory_barrier()  smp_mb()
 
@@ -72,14 +72,14 @@
 }
 
 static inline unsigned long
-xnarch_atomic_cmpxchg(xnarch_atomic_t *v, unsigned long old, unsigned long new)
+xnarch_atomic_cmpxchg(xnarch_atomic_t *v, unsigned long old, unsigned long New)
 {
 	volatile void *ptr = &v->counter;
 	unsigned long prev;
 
 	__asm__ __volatile__(LOCK_PREFIX "cmpxchgl %1,%2"
 			     : "=a"(prev)
-			     : "r"(new), "m"(*__xeno_xg(ptr)), "0"(old)
+			     : "r"(New), "m"(*__xeno_xg(ptr)), "0"(old)
 			     : "memory");
 	return prev;
 }
